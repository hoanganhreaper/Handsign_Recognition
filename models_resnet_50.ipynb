{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from ResNet import ResNet50\n",
    "from split_asl_alphabet.dataset import ASLDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPUS = [0]\n",
    "EPOCHS = 10\n",
    "NO_CLASSES = 29\n",
    "TRAIN_DIR = 'split_asl_alphabet/train'\n",
    "VAL_DIR = 'split_asl_alphabet/test'\n",
    "IMG_DIM = 226\n",
    "BATCH_SIZE = 128\n",
    "L_RATE = 0.01\n",
    "W_DECAY = 0.0005\n",
    "MOMENTUM = 0.9\n",
    "CHECKPOINT_DIR = 'checkpoints'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_DIM),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5,0.5], std=[0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataLoader's with batch size 128 and 20 workers.\n",
      "train dataset size :48488 images \n",
      "test dataset size :9483 images \n"
     ]
    }
   ],
   "source": [
    "train_data_simple = ASLDataset(data_path=TRAIN_DIR,range_index = [1,2500], transform=data_transform)\n",
    "test_data_simple = ASLDataset(data_path=TRAIN_DIR,range_index = [2500,3000], transform=data_transform)\n",
    "\n",
    "# Setup batch size and number of workers \n",
    "NUM_WORKERS = os.cpu_count()\n",
    "print(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")\n",
    "\n",
    "# Create DataLoader's\n",
    "train_dataloader_simple = DataLoader(train_data_simple, \n",
    "                                     batch_size=BATCH_SIZE, \n",
    "                                     shuffle=True, \n",
    "                                     num_workers=NUM_WORKERS)\n",
    "\n",
    "test_dataloader_simple = DataLoader(test_data_simple, \n",
    "                                    batch_size=BATCH_SIZE, \n",
    "                                    shuffle=False, \n",
    "                                    num_workers=NUM_WORKERS)\n",
    "print(f'train dataset size :{len(train_data_simple)} images ')\n",
    "print(f'test dataset size :{len(test_data_simple)} images ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet50(29).to('cpu')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader_simple):\n\u001b[0;32m      5\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m inp\n\u001b[1;32m----> 6\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m), labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m net(inputs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    losses = []\n",
    "    running_loss = 0\n",
    "    for i, inp in enumerate(train_dataloader_simple):\n",
    "        inputs= inp\n",
    "        inputs, labels = inputs.to('cpu'), labels.to('cpu')\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i%100 == 0 and i > 0:\n",
    "            print(f'Loss [{epoch+1}, {i}](epoch, minibatch): ', running_loss)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    avg_loss = sum(losses)/len(losses)\n",
    "    scheduler.step(avg_loss)\n",
    "            \n",
    "print('Training Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inpt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader_simple):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Assuming inpt is a tuple with input data and labels\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m inpt\n\u001b[0;32m      8\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# refreshing gradients\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "total_steps = 1\n",
    "for epoch in range(EPOCHS):\n",
    "    losses = []\n",
    "    running_loss = 0\n",
    "    for step, inpt in enumerate(train_dataloader_simple):\n",
    "        # Assuming inpt is a tuple with input data and labels\n",
    "        X, y = inpt\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # refreshing gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        pred = net(X)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = criterion(pred, y)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if total_steps % 10 == 0:\n",
    "            print(f'Loss [{epoch+1}, {step+1}](epoch, minibatch): ', running_loss / 10)\n",
    "            running_loss = 0.0\n",
    "        \n",
    "        total_steps += 1\n",
    "    \n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "print('Training Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader_simple:\n",
    "        images, labels = data\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "        outputs = net(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy on 10,000 test images: ', 100*(correct/total), '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectbme2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
